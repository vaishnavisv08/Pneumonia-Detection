{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Pneumonia?\n",
    "**Pneumonia is an inflammatory condition of the lung affecting primarily the small air sacs known as alveoli.Symptoms typically include some combination of productive or dry cough, chest pain, fever and difficulty breathing. The severity of the condition is variable. Pneumonia is usually caused by infection with viruses or bacteria and less commonly by other microorganisms, certain medications or conditions such as autoimmune diseases.Risk factors include cystic fibrosis, chronic obstructive pulmonary disease (COPD), asthma, diabetes, heart failure, a history of smoking, a poor ability to cough such as following a stroke and a weak immune system. Diagnosis is often based on symptoms and physical examination. Chest X-ray, blood tests, and culture of the sputum may help confirm the diagnosis.The disease may be classified by where it was acquired, such as community- or hospital-acquired or healthcare-associated pneumonia.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  #  visualizations like histograms, scatter plots, etc \n",
    "import seaborn as sns  #  to visualize random distributions\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import cv2  #pip install opencv-python # powerful library for computer vision tasks such as image processing, etc \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the Pneumonia Dataset\n",
    "**The dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (Pneumonia/Normal). There are 5,863 X-Ray images (JPEG) and 2 categories (Pneumonia/Normal).\n",
    "Chest X-ray images (anterior-posterior) were selected from retrospective cohorts of pediatric patients. All chest X-ray imaging was performed as part of patients’ routine clinical care.\n",
    "For the analysis of chest x-ray images, all chest radiographs were initially screened for quality control by removing all low quality or unreadable scans. The diagnoses for the images were then graded by two expert physicians before being cleared for training the AI system. In order to account for any grading errors, the evaluation set was also checked by a third expert.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function preprocess_data processes a dataset of chest X-ray images, converting them to grayscale, resizing them to 150x150 pixels, and labeling them as either pneumonia or normal based on their directory. The output is a list of image-label pairs, which can then be used for training a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['PNEUMONIA', 'NORMAL']\n",
    "img_size = 150\n",
    "def preprocess_data(data_dir):\n",
    "    data = [] \n",
    "    for label in labels: \n",
    "        path = os.path.join(data_dir, label)\n",
    "        class_num = labels.index(label)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                #print(np.shape(img_arr))\n",
    "                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n",
    "                #print(np.shape(resized_arr))\n",
    "                data.append([resized_arr, class_num]) \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "train = preprocess_data(r'E:\\Dataset_chest_xray/train')\n",
    "print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = preprocess_data(r'E:\\Dataset_chest_xray/test')\n",
    "val = preprocess_data(r'E:\\Dataset_chest_xray/val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list to a pandas Series\n",
    "l = []\n",
    "for i in train:\n",
    "    if i[1] == 0:\n",
    "        l.append(\"Pneumonia\")\n",
    "    else:\n",
    "        l.append(\"Normal\")\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "label_df = pd.DataFrame(l, columns=['Label'])\n",
    "print(label_df.head())\n",
    "\n",
    "# Plot using Seaborn\n",
    "sns.set_style('darkgrid')\n",
    "sns.countplot(x='Label', data=label_df)\n",
    "plt.title(\"Distribution of Labels in the Training Set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The data seems imbalanced . To increase the no. of training examples, we will use data augmentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Previewing the images of both the classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.imshow(train[0][0], cmap='gray')\n",
    "plt.title(labels[train[0][1]])\n",
    "\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.imshow(train[-1][0], cmap='gray')\n",
    "plt.title(labels[train[-1][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "x_val = []\n",
    "y_val = []\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for feature, label in train:\n",
    "    x_train.append(feature)\n",
    "    y_train.append(label)\n",
    "\n",
    "for feature, label in test:\n",
    "    x_test.append(feature)\n",
    "    y_test.append(label)\n",
    "    \n",
    "for feature, label in val:\n",
    "    x_val.append(feature)\n",
    "    y_val.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We perform a grayscale normalization to reduce the effect of illumination's differences.Moreover the CNN converges faster on [0..1] data than on [0..255].**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "x_train = np.array(x_train) / 255\n",
    "x_val = np.array(x_val) / 255\n",
    "x_test = np.array(x_test) / 255\n",
    "\n",
    "print(x_train[0])\n",
    "print(np.shape(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reshape foramat -->reshape(number_of_images/batch_size, height, width, channels); \n",
    "-1 --> sets the number of images in x_train while maintaining the overall array size\n",
    "150,150 --> image size; \n",
    "1 --> indicates the number of channels in the image. \n",
    "For grayscale images, there is only one channel. \n",
    "If the images were in RGB format, this value would be 3 (for red, green, and blue channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize data for deep learning (CNN models)\n",
    "\n",
    "x_train = x_train.reshape(-1, img_size, img_size, 1)\n",
    "y_train = np.array(y_train)\n",
    "print(np.shape(x_train))\n",
    "print(x_train[0])\n",
    "\n",
    "x_val = x_val.reshape(-1, img_size, img_size, 1)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "x_test = x_test.reshape(-1, img_size, img_size, 1)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "**In order to avoid overfitting problem, we need to expand artificially our dataset. We can make your existing dataset even larger. The idea is to alter the training data with small transformations to reproduce the variations.\n",
    "Approaches that alter the training data in ways that change the array representation while keeping the label the same are known as data augmentation techniques. Some popular augmentations people use are grayscales, horizontal flips, vertical flips, random crops, color jitters, translations, rotations, and much more.\n",
    "By applying just a couple of these transformations to our training data, we can easily double or triple the number of training examples and create a very robust model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With data augmentation to prevent overfitting and handling the imbalance in dataset\n",
    "# no change has been done for this dataset\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1) \n",
    "\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the data augmentation, i choosed to :\n",
    "1. Randomly rotate some training images by 30 degrees \n",
    "2. Randomly Zoom by 20% some training images\n",
    "3. Randomly shift images horizontally by 10% of the width \n",
    "4. Randomly shift images vertically by 10% of the height \n",
    "5. Randomly flip images horizontally.\n",
    "Once our model is ready, we fit the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starts a NN where layers are added one after another.\n",
    "model = Sequential()\n",
    "\n",
    "# First Conv2D Layer:\n",
    "# Adds a 2D convolution layer (Conv2D) with 32 filters, each of size (3,3). \n",
    "# It uses ReLU as the activation function, and the input data shape is (150,150,1) (grayscale image).\n",
    "# BatchNormalization() normalizes the data to help the model learn faster.\n",
    "# MaxPool2D reduces the size of the data by pooling, which helps focus on important features \n",
    "# (reduces data by half with strides of 2).\n",
    "\n",
    "model.add(Conv2D(32 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (150,150,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "\n",
    "# Second Conv2D Layer:\n",
    "# Adds a 2D convolution layer with 64 filters and similar parameters.\n",
    "# Dropout(0.1) drops out 10% of the neurons randomly to prevent overfitting.\n",
    "# Followed by BatchNormalization() and MaxPool2D() as before.\n",
    "\n",
    "model.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "\n",
    "# Third Conv2D Layer:\n",
    "# Adds another Conv2D layer with 64 filters.\n",
    "# BatchNormalization() and MaxPool2D() for normalization and downsampling.\n",
    "\n",
    "model.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "\n",
    "# Fourth Conv2D Layer:\n",
    "# Adds a Conv2D layer with 128 filters, followed by Dropout(0.2), BatchNormalization(), and MaxPool2D().\n",
    "\n",
    "model.add(Conv2D(128 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "\n",
    "# Fifth Conv2D Layer:\n",
    "# Adds a Conv2D layer with 256 filters, followed by Dropout(0.2), BatchNormalization(), and MaxPool2D().\n",
    "\n",
    "model.add(Conv2D(256 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "\n",
    "# Flattening the Data:\n",
    "# Flatten() transforms the 2D data into 1D, which is necessary for the next dense layers.\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully Connected (Dense) Layers:\n",
    "# adds a fully connected layer with 128 units and ReLU activation.\n",
    "# Dropout(0.2) randomly drops 20% of neurons.\n",
    "\n",
    "model.add(Dense(units = 128 , activation = 'relu'))\n",
    "model.add(Dropout(0.2)) \n",
    "\n",
    "# adds a final output layer with a single neuron using a \n",
    "# sigmoid activation for binary classification.\n",
    "model.add(Dense(units = 1 , activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the Model:\n",
    "# The model is compiled using the RMSprop optimizer, binary_crossentropy loss function\n",
    "# (used for binary classification), and accuracy as the metric to track performance.\n",
    "model.compile(optimizer = \"rmsprop\" , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n",
    "\n",
    "# Summary:\n",
    "# prints a summary of the model architecture, showing each layer’s parameters and output shapes.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model watches the validation accuracy (monitor='val_accuracy').\n",
    "If accuracy doesn’t improve for 2 epochs (patience=2), the learning rate is reduced by 30% (factor=0.3).\n",
    "The model won't lower the learning rate below 0.000001 (min_lr=0.000001).\n",
    "This makes training slower but helps the model learn better in the long run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.3, min_lr=0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model trains on batches of 32 augmented images at a time.\n",
    "The training runs for 12 epochs, processing the entire dataset 12 times.\n",
    "After each epoch, the model is evaluated on the validation data to check its progress.\n",
    "If the validation accuracy doesn’t improve for 2 epochs, the learning rate is reduced by 30% to help the model fine-tune itself.\n",
    "This process helps the model gradually improve its ability to detect pneumonia from chest X-ray images while preventing overfitting and ensuring stable learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(datagen.flow(x_train,y_train, batch_size = 32) ,epochs = 12 , validation_data = datagen.flow(x_val, y_val) ,callbacks = [learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss of the model is - \" , model.evaluate(x_test,y_test)[0])\n",
    "print(\"Accuracy of the model is - \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis after Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [i for i in range(12)]\n",
    "fig , ax = plt.subplots(1,2)\n",
    "train_acc = history.history['accuracy']\n",
    "train_loss = history.history['loss']\n",
    "val_acc = history.history['val_accuracy']\n",
    "val_loss = history.history['val_loss']\n",
    "fig.set_size_inches(20,10)\n",
    "\n",
    "ax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\n",
    "ax[0].plot(epochs , val_acc , 'ro-' , label = 'Validation Accuracy')\n",
    "ax[0].set_title('Training & Validation Accuracy')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "ax[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "ax[1].plot(epochs , train_loss , 'g-o' , label = 'Training Loss')\n",
    "ax[1].plot(epochs , val_loss , 'r-o' , label = 'Validation Loss')\n",
    "ax[1].set_title('Testing Accuracy & Loss')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "ax[1].set_ylabel(\"Training & Validation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =(model.predict(x_test) > 0.5).astype(\"int32\") #binary classification\n",
    "predictions = predictions.reshape(1,-1)[0]\n",
    "predictions[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions, target_names = ['Pneumonia (Class 0)','Normal (Class 1)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,predictions)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.DataFrame(cm , index = ['0','1'] , columns = ['0','1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='',xticklabels = labels,yticklabels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = np.nonzero(predictions == y_test)[0]\n",
    "incorrect = np.nonzero(predictions != y_test)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some of the Correctly Predicted Classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for c in correct[:6]:\n",
    "    plt.subplot(3,2,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(x_test[c].reshape(150,150), cmap=\"gray\", interpolation='none')\n",
    "    plt.title(\"Predicted Class {},Actual Class {}\".format(predictions[c], y_test[c]))\n",
    "    plt.tight_layout()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some of the Incorrectly Predicted Classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for c in incorrect[:6]:\n",
    "    plt.subplot(3,2,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(x_test[c].reshape(150,150), cmap=\"gray\", interpolation='none')\n",
    "    plt.title(\"Predicted Class {},Actual Class {}\".format(predictions[c], y_test[c]))\n",
    "    plt.tight_layout()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
